{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from add_label import buildBST, getGRCIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('NCSU-DigIC-GraphData-2023-07-25/xbar/1/xbar.json.gz','rb') as f:\n",
    "    design = json.loads(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = pd.DataFrame(design['instances'])\n",
    "nets = pd.DataFrame(design['nets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=np.load('NCSU-DigIC-GraphData-2023-07-25/xbar/1/xbar_connectivity.npz')\n",
    "A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "A = A.__mul__(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>xloc</th>\n",
       "      <th>yloc</th>\n",
       "      <th>cell</th>\n",
       "      <th>orient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clk_gate_out_reg/latch</td>\n",
       "      <td>0</td>\n",
       "      <td>41984</td>\n",
       "      <td>44544</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clk_gate_out_reg_0/latch</td>\n",
       "      <td>1</td>\n",
       "      <td>41984</td>\n",
       "      <td>47616</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clk_gate_out_reg_1/latch</td>\n",
       "      <td>2</td>\n",
       "      <td>44160</td>\n",
       "      <td>44544</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clk_gate_out_reg_2/latch</td>\n",
       "      <td>3</td>\n",
       "      <td>44160</td>\n",
       "      <td>47616</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clk_gate_out_reg_3/latch</td>\n",
       "      <td>4</td>\n",
       "      <td>46336</td>\n",
       "      <td>47616</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>U4123</td>\n",
       "      <td>3947</td>\n",
       "      <td>21888</td>\n",
       "      <td>53760</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>U4125</td>\n",
       "      <td>3948</td>\n",
       "      <td>33664</td>\n",
       "      <td>66048</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>U4128</td>\n",
       "      <td>3949</td>\n",
       "      <td>23296</td>\n",
       "      <td>66048</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>ZCTSBUF_205_132</td>\n",
       "      <td>3950</td>\n",
       "      <td>40576</td>\n",
       "      <td>44544</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>ZCTSBUF_466_133</td>\n",
       "      <td>3951</td>\n",
       "      <td>46848</td>\n",
       "      <td>44544</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3952 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name    id   xloc   yloc  cell  orient\n",
       "0       clk_gate_out_reg/latch     0  41984  44544    23       0\n",
       "1     clk_gate_out_reg_0/latch     1  41984  47616    23       6\n",
       "2     clk_gate_out_reg_1/latch     2  44160  44544    23       0\n",
       "3     clk_gate_out_reg_2/latch     3  44160  47616    23       0\n",
       "4     clk_gate_out_reg_3/latch     4  46336  47616    23       0\n",
       "...                        ...   ...    ...    ...   ...     ...\n",
       "3947                     U4123  3947  21888  53760    42       4\n",
       "3948                     U4125  3948  33664  66048    42       0\n",
       "3949                     U4128  3949  23296  66048    34       0\n",
       "3950           ZCTSBUF_205_132  3950  40576  44544    11       0\n",
       "3951           ZCTSBUF_466_133  3951  46848  44544    11       6\n",
       "\n",
       "[3952 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBST(array,start=0,finish=-1):\n",
    "    if finish<0:\n",
    "        finish = len(array)\n",
    "    mid = (start + finish) // 2\n",
    "    if mid-start==1:\n",
    "        ltl=start\n",
    "    else:\n",
    "        ltl=buildBST(array,start,mid)\n",
    "    \n",
    "    if finish-mid==1:\n",
    "        gtl=mid\n",
    "    else:\n",
    "        gtl=buildBST(array,mid,finish)\n",
    "        \n",
    "    return((array[mid],ltl,gtl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congestion_data = np.load('NCSU-DigIC-GraphData-2023-07-25/xbar/1/xbar_congestion.npz')\n",
    "xbst = buildBST(congestion_data['xBoundaryList'])\n",
    "ybst = buildBST(congestion_data['yBoundaryList'])\n",
    "demand = np.zeros(shape = [instances.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGRCIndex(x,y,xbst,ybst):\n",
    "    while (type(xbst)==tuple):\n",
    "        if x < xbst[0]:\n",
    "            xbst=xbst[1]\n",
    "        else:\n",
    "            xbst=xbst[2]\n",
    "            \n",
    "    while (type(ybst)==tuple):\n",
    "        if y < ybst[0]:\n",
    "            ybst=ybst[1]\n",
    "        else:\n",
    "            ybst=ybst[2]\n",
    "            \n",
    "    return ybst, xbst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(instances.shape[0]):\n",
    "    # print(k)\n",
    "    xloc = instances.iloc[k]['xloc']; yloc = instances.iloc[k]['yloc']\n",
    "    i,j=getGRCIndex(xloc,yloc,xbst,ybst)\n",
    "    d = 0 \n",
    "    for l in list(congestion_data['layerList']): \n",
    "        lyr=list(congestion_data['layerList']).index(l)\n",
    "        d += congestion_data['demand'][lyr][i][j]\n",
    "    demand[k] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances['routing_demand'] = demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use randomly selected 70% instances as training and 30% instances as test\n",
    "np.random.seed(42)\n",
    "train, test= train_test_split(instances, test_size=0.3, random_state=42)\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.tensor(train[['xloc', 'yloc', 'cell', 'orient']].values, dtype=torch.float)\n",
    "num_nodes = len(node_features)\n",
    "edge_index = torch.tensor([[i for i in range(num_nodes)], [i for i in range(num_nodes)]], dtype=torch.long)\n",
    "data = Data(x=node_features, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCNConv(in_channels=4, hidden_channels=64, out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2766])) that is different to the input size (torch.Size([2766, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 1319650944.0\n",
      "Epoch 1001/5000, Loss: 866.3240966796875\n",
      "Epoch 2001/5000, Loss: 713.2647705078125\n",
      "Epoch 3001/5000, Loss: 508.3110046386719\n",
      "Epoch 4001/5000, Loss: 298.0909729003906\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Assuming you have labels (target values) for a supervised task\n",
    "    # Replace 'labels_column' with the actual column containing your labels\n",
    "    labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testmodel():\n",
    "    model.eval()\n",
    "    output = model(node_features, edge_index)\n",
    "    loss_test = criterion(output[test.index], labels[test.index])\n",
    "    print(\"Test set results:\",\n",
    "          \"MSE loss= {:.4f}\".format(loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: MSE loss= 145.4440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1186])) that is different to the input size (torch.Size([1186, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "testmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 242283440.0\n",
      "Epoch 1001/5000, Loss: 72.3841552734375\n",
      "Epoch 2001/5000, Loss: 70.42475891113281\n",
      "Epoch 3001/5000, Loss: 68.26179504394531\n",
      "Epoch 4001/5000, Loss: 2407.57763671875\n",
      "Test set results: MSE loss= 67.5126\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 2\n",
    "hidden_dimensions = [64, 32]\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimensions, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add input layer\n",
    "        self.layers.append(GCNConv(in_channels, hidden_dimensions[0]))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            self.layers.append(GCNConv(hidden_dimensions[i-1], hidden_dimensions[i]))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.layers.append(GCNConv(hidden_dimensions[-1], out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = GCNModel(in_channels=4, hidden_dimensions=hidden_dimensions, out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Assuming you have labels (target values) for a supervised task\n",
    "    # Replace 'labels_column' with the actual column containing your labels\n",
    "    labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "def testmodel():\n",
    "    model.eval()\n",
    "    output = model(node_features, edge_index)\n",
    "    loss_test = criterion(output[test.index], labels[test.index])\n",
    "    print(\"Test set results:\",\n",
    "          \"MSE loss= {:.4f}\".format(loss_test.item()))\n",
    "\n",
    "testmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 93479648.0\n",
      "Epoch 1001/5000, Loss: 75.83006286621094\n",
      "Epoch 2001/5000, Loss: 74.41992950439453\n",
      "Epoch 3001/5000, Loss: 72.35708618164062\n",
      "Epoch 4001/5000, Loss: 69.81207275390625\n",
      "Test set results: MSE loss= 69.0787\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 3  # Change this to the desired number of hidden layers\n",
    "hidden_dimensions = [64, 32, 16]  # Example: Three hidden layers with 64, 32, and 16 hidden units\n",
    "\n",
    "# Build the GCN model with multiple layers\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimensions, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add input layer\n",
    "        self.layers.append(GCNConv(in_channels, hidden_dimensions[0]))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            self.layers.append(GCNConv(hidden_dimensions[i-1], hidden_dimensions[i]))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.layers.append(GCNConv(hidden_dimensions[-1], out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = GCNModel(in_channels=4, hidden_dimensions=hidden_dimensions, out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Assuming you have labels (target values) for a supervised task\n",
    "    # Replace 'labels_column' with the actual column containing your labels\n",
    "    labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "def testmodel():\n",
    "    model.eval()\n",
    "    output = model(node_features, edge_index)\n",
    "    loss_test = criterion(output[test.index], labels[test.index])\n",
    "    print(\"Test set results:\",\n",
    "          \"MSE loss= {:.4f}\".format(loss_test.item()))\n",
    "\n",
    "testmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 668022784.0\n",
      "Epoch 1001/5000, Loss: 73.16777038574219\n",
      "Epoch 2001/5000, Loss: 72.20680236816406\n",
      "Epoch 3001/5000, Loss: 70.7574462890625\n",
      "Epoch 4001/5000, Loss: 68.8647232055664\n",
      "Test set results: MSE loss= 68.5035\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 4  # Change this to the desired number of hidden layers\n",
    "hidden_dimensions = [64, 32, 16, 8]  # Example: Three hidden layers with 64, 32, and 16 hidden units\n",
    "\n",
    "# Build the GCN model with multiple layers\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimensions, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add input layer\n",
    "        self.layers.append(GCNConv(in_channels, hidden_dimensions[0]))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            self.layers.append(GCNConv(hidden_dimensions[i-1], hidden_dimensions[i]))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.layers.append(GCNConv(hidden_dimensions[-1], out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = GCNModel(in_channels=4, hidden_dimensions=hidden_dimensions, out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Assuming you have labels (target values) for a supervised task\n",
    "    # Replace 'labels_column' with the actual column containing your labels\n",
    "    labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "def testmodel():\n",
    "    model.eval()\n",
    "    output = model(node_features, edge_index)\n",
    "    loss_test = criterion(output[test.index], labels[test.index])\n",
    "    print(\"Test set results:\",\n",
    "          \"MSE loss= {:.4f}\".format(loss_test.item()))\n",
    "\n",
    "testmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 226736464.0\n",
      "Epoch 1001/5000, Loss: 93.14885711669922\n",
      "Epoch 2001/5000, Loss: 88.8634033203125\n",
      "Epoch 3001/5000, Loss: 82.4587173461914\n",
      "Epoch 4001/5000, Loss: 74.42916870117188\n",
      "Test set results: MSE loss= 68.4683\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 5  # Change this to the desired number of hidden layers\n",
    "hidden_dimensions = [64, 32, 16, 8, 4]  # Example: Three hidden layers with 64, 32, and 16 hidden units\n",
    "\n",
    "# Build the GCN model with multiple layers\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimensions, out_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Add input layer\n",
    "        self.layers.append(GCNConv(in_channels, hidden_dimensions[0]))\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, num_hidden_layers):\n",
    "            self.layers.append(GCNConv(hidden_dimensions[i-1], hidden_dimensions[i]))\n",
    "        \n",
    "        # Add output layer\n",
    "        self.layers.append(GCNConv(hidden_dimensions[-1], out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = GCNModel(in_channels=4, hidden_dimensions=hidden_dimensions, out_channels=1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Assuming you have labels (target values) for a supervised task\n",
    "    # Replace 'labels_column' with the actual column containing your labels\n",
    "    labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "def testmodel():\n",
    "    model.eval()\n",
    "    output = model(node_features, edge_index)\n",
    "    loss_test = criterion(output[test.index], labels[test.index])\n",
    "    print(\"Test set results:\",\n",
    "          \"MSE loss= {:.4f}\".format(loss_test.item()))\n",
    "\n",
    "testmodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My implementation of this model shows that 2 layers has the lowest MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 418328.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=47'>48</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=49'>50</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=50'>51</a>\u001b[0m output \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=52'>53</a>\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=53'>54</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb Cell 26'\u001b[0m in \u001b[0;36mGCNWithAttention.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Swap batch and sequence dimensions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=14'>15</a>\u001b[0m \u001b[39m# Compute attention scores and apply attention\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=15'>16</a>\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(x, x, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=17'>18</a>\u001b[0m \u001b[39m# Transpose back to the original shape\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lisahwang/school/dsc180a/assignment3/assignment3.ipynb#ch0000060?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1226'>1227</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1227'>1228</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1228'>1229</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1237'>1238</a>\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1238'>1239</a>\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1239'>1240</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1240'>1241</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1241'>1242</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1242'>1243</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1243'>1244</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1244'>1245</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1245'>1246</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1246'>1247</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1247'>1248</a>\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1248'>1249</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1249'>1250</a>\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1250'>1251</a>\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1251'>1252</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=1252'>1253</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py:5405\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py?line=5402'>5403</a>\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py?line=5403'>5404</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py?line=5404'>5405</a>\u001b[0m     attn_output_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q_scaled, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py?line=5405'>5406</a>\u001b[0m attn_output_weights \u001b[39m=\u001b[39m softmax(attn_output_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/lisahwang/opt/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/functional.py?line=5406'>5407</a>\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GCNWithAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads=2):\n",
    "        super(GCNWithAttention, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "        self.attention = nn.MultiheadAttention(out_channels, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Perform GCN message passing\n",
    "        x = self.conv(x, edge_index)\n",
    "        \n",
    "        # Transpose x to match the expected shape for attention\n",
    "        x = x.unsqueeze(0)  # Add a batch dimension\n",
    "        x = x.transpose(0, 1)  # Swap batch and sequence dimensions\n",
    "        \n",
    "        # Compute attention scores and apply attention\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        \n",
    "        # Transpose back to the original shape\n",
    "        x = x.transpose(0, 1).squeeze(0)\n",
    "        \n",
    "        x = F.relu(x)  # Apply non-linearity\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "in_channels = 4  # Change this based on the number of input features\n",
    "out_channels = 2766  # Change this based on the desired output dimension\n",
    "num_heads = 2\n",
    "model = GCNWithAttention(in_channels, out_channels, num_heads)\n",
    "\n",
    "# Assuming you have a data instance named data\n",
    "output = model(data.x, data.edge_index)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Assuming 'train_data' contains the node features and edge indices\n",
    "data = Data(x=torch.tensor(train[['xloc', 'yloc', 'cell', 'orient']].values, dtype=torch.float),\n",
    "            edge_index=torch.tensor([[i for i in range(num_nodes)], [i for i in range(num_nodes)]], dtype=torch.long))\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(train['routing_demand'].values, dtype=torch.float)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "690cf5439abac397a7a092cd64e6e605b431e3292baa175258c5e15794f18b99"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dsc80')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
