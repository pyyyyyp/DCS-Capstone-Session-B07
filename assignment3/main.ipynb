{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, ModuleList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('xbar/1/xbar.json.gz','rb') as f:\n",
    "    design = json.loads(f.read().decode('utf-8'))\n",
    "\n",
    "instances = pd.DataFrame(design['instances'])\n",
    "nets = pd.DataFrame(design['nets'])\n",
    "\n",
    "conn=np.load('xbar/1/xbar_connectivity.npz')\n",
    "A = coo_matrix((conn['data'], (conn['row'], conn['col'])), shape=conn['shape'])\n",
    "A = A.__mul__(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBST(array,start=0,finish=-1):\n",
    "    if finish<0:\n",
    "        finish = len(array)\n",
    "    mid = (start + finish) // 2\n",
    "    if mid-start==1:\n",
    "        ltl=start\n",
    "    else:\n",
    "        ltl=buildBST(array,start,mid)\n",
    "    \n",
    "    if finish-mid==1:\n",
    "        gtl=mid\n",
    "    else:\n",
    "        gtl=buildBST(array,mid,finish)\n",
    "        \n",
    "    return((array[mid],ltl,gtl))\n",
    "\n",
    "congestion_data = np.load('xbar/1/xbar_congestion.npz')\n",
    "xbst=buildBST(congestion_data['xBoundaryList'])\n",
    "ybst=buildBST(congestion_data['yBoundaryList'])\n",
    "demand = np.zeros(shape = [instances.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGRCIndex(x,y,xbst,ybst):\n",
    "    while (type(xbst)==tuple):\n",
    "        if x < xbst[0]:\n",
    "            xbst=xbst[1]\n",
    "        else:\n",
    "            xbst=xbst[2]\n",
    "            \n",
    "    while (type(ybst)==tuple):\n",
    "        if y < ybst[0]:\n",
    "            ybst=ybst[1]\n",
    "        else:\n",
    "            ybst=ybst[2]\n",
    "            \n",
    "    return ybst, xbst\n",
    "\n",
    "\n",
    "for k in range(instances.shape[0]):\n",
    "    # print(k)\n",
    "    xloc = instances.iloc[k]['xloc']; yloc = instances.iloc[k]['yloc']\n",
    "    i,j=getGRCIndex(xloc,yloc,xbst,ybst)\n",
    "    d = 0 \n",
    "    for l in list(congestion_data['layerList']): \n",
    "        lyr=list(congestion_data['layerList']).index(l)\n",
    "        d += congestion_data['demand'][lyr][i][j]\n",
    "    demand[k] = d\n",
    "        \n",
    "instances['routing_demand'] = demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # for replication\n",
    "\n",
    "X = torch.tensor(instances[['xloc', 'yloc', 'cell', 'orient']].values) # 4 features\n",
    "y = torch.tensor(instances['routing_demand'].values) # y value\n",
    "\n",
    "# getting edge index for message passing\n",
    "ei = from_scipy_sparse_matrix(A)\n",
    "edge_index = ei[0]\n",
    "\n",
    "data = Data(x=X, edge_index=edge_index, y=y)\n",
    "\n",
    "# Use RandomNodeSplit to split nodes randomly\n",
    "split = RandomNodeSplit(num_splits=1, num_val=0.0, num_test=0.3)\n",
    "split_data = split(data)\n",
    "\n",
    "# we need to split edge_index based off how node was split\n",
    "train = torch.argwhere(split_data.train_mask).reshape(-1)\n",
    "test = torch.argwhere(split_data.test_mask).reshape(-1)\n",
    "train_mapping = {a.item():b for a,b in zip(train, range(train.shape[0]))}\n",
    "test_mapping = {a.item():b for a,b in zip(test, range(test.shape[0]))}\n",
    "X_train = split_data.x[split_data.train_mask].float()\n",
    "X_test = split_data.x[split_data.test_mask].float()\n",
    "y_train = split_data.y[split_data.train_mask].float()\n",
    "y_test = split_data.y[split_data.test_mask].float()\n",
    "edge_index_train = torch.tensor([train_mapping[i.item()] for i in edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.train_mask).reshape(-1)), dim=0)).reshape(-1)].reshape(-1)]).reshape(2, edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.train_mask).reshape(-1)), dim=0)).reshape(-1)].shape[1])\n",
    "edge_index_test = torch.tensor([test_mapping[i.item()] for i in edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.test_mask).reshape(-1)), dim=0)).reshape(-1)].reshape(-1)]).reshape(2, edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.test_mask).reshape(-1)), dim=0)).reshape(-1)].shape[1])\n",
    "final_train = Data(x=X_train, edge_index=edge_index_train, y=y_train)\n",
    "final_test = Data(x=X_test, edge_index=edge_index_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2766, 768])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = GCNConv(4, 768)\n",
    "conv1(split_data.x[split_data.train_mask].float(), torch.tensor([mapping[i.item()] for i in edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.train_mask).reshape(-1)), dim=0)).reshape(-1)].reshape(-1)]).reshape(2, edge_index[:,torch.argwhere(torch.all(torch.isin(edge_index, torch.argwhere(split_data.train_mask).reshape(-1)), dim=0)).reshape(-1)].shape[1])\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.gcn = GCNConv(4, 768)\n",
    "        self.hidden_layers = ModuleList([GCNConv(768, 768) for _ in range(self.num_hidden_layers)])\n",
    "        self.out = Linear(768, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.gcn(x, edge_index).relu()\n",
    "        for _, l in enumerate(self.hidden_layers):\n",
    "            h = l(h, edge_index).relu()\n",
    "        z = self.out(h)\n",
    "        return h, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1a\n",
    "\n",
    "Below I set up a train function that takes in a model and a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    embeddings = []\n",
    "    losses = []\n",
    "    outputs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h, z = model(data.x, data.edge_index)\n",
    "\n",
    "        loss = criterion(z.float(), data.y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        embeddings.append(h)\n",
    "        losses.append(loss)\n",
    "\n",
    "        outputs.append(z.argmax(dim=1))\n",
    "\n",
    "        # Print metrics every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch:>3} | Loss: {loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 1333273.75\n",
      "Epoch  10 | Loss: 520175.38\n",
      "Epoch  20 | Loss: 2042.67\n",
      "Epoch  30 | Loss: 237.68\n",
      "Epoch  40 | Loss: 618.19\n",
      "Epoch  50 | Loss: 631.04\n",
      "Epoch  60 | Loss: 525.16\n",
      "Epoch  70 | Loss: 267.45\n",
      "Epoch  80 | Loss: 203.47\n",
      "Epoch  90 | Loss: 180.41\n",
      "Epoch 100 | Loss: 161.00\n",
      "Epoch 110 | Loss: 141.21\n",
      "Epoch 120 | Loss: 121.49\n",
      "Epoch 130 | Loss: 103.20\n",
      "Epoch 140 | Loss: 83.89\n",
      "Epoch 150 | Loss: 69.90\n",
      "Epoch 160 | Loss: 63.53\n",
      "Epoch 170 | Loss: 61.43\n",
      "Epoch 180 | Loss: 60.96\n",
      "Epoch 190 | Loss: 60.92\n"
     ]
    }
   ],
   "source": [
    "one_layer_model = GCN(0)\n",
    "train(one_layer_model, final_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 11175.13\n",
      "Epoch  10 | Loss: 25486240.00\n",
      "Epoch  20 | Loss: 24455.21\n",
      "Epoch  30 | Loss: 357154.22\n",
      "Epoch  40 | Loss: 38370.19\n",
      "Epoch  50 | Loss: 9734.55\n",
      "Epoch  60 | Loss: 1827.83\n",
      "Epoch  70 | Loss: 682.14\n",
      "Epoch  80 | Loss: 623.60\n",
      "Epoch  90 | Loss: 588.38\n",
      "Epoch 100 | Loss: 491.25\n",
      "Epoch 110 | Loss: 391.73\n",
      "Epoch 120 | Loss: 322.60\n",
      "Epoch 130 | Loss: 266.27\n",
      "Epoch 140 | Loss: 213.38\n",
      "Epoch 150 | Loss: 170.73\n",
      "Epoch 160 | Loss: 137.79\n",
      "Epoch 170 | Loss: 113.86\n",
      "Epoch 180 | Loss: 97.98\n",
      "Epoch 190 | Loss: 88.28\n"
     ]
    }
   ],
   "source": [
    "two_layer_model = GCN(1)\n",
    "train(two_layer_model, final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 7228.60\n",
      "Epoch  10 | Loss: 164106912.00\n",
      "Epoch  20 | Loss: 120014176.00\n",
      "Epoch  30 | Loss: 38614256.00\n",
      "Epoch  40 | Loss: 11552405.00\n",
      "Epoch  50 | Loss: 4134028.75\n",
      "Epoch  60 | Loss: 1049274.75\n",
      "Epoch  70 | Loss: 75680.92\n",
      "Epoch  80 | Loss: 51247.93\n",
      "Epoch  90 | Loss: 93241.73\n",
      "Epoch 100 | Loss: 14497.59\n",
      "Epoch 110 | Loss: 13358.26\n",
      "Epoch 120 | Loss: 7522.93\n",
      "Epoch 130 | Loss: 6973.73\n",
      "Epoch 140 | Loss: 5439.39\n",
      "Epoch 150 | Loss: 5271.04\n",
      "Epoch 160 | Loss: 4910.72\n",
      "Epoch 170 | Loss: 4585.75\n",
      "Epoch 180 | Loss: 4322.17\n",
      "Epoch 190 | Loss: 4082.26\n"
     ]
    }
   ],
   "source": [
    "three_layer_model = GCN(2)\n",
    "train(three_layer_model, final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 356.64\n",
      "Epoch  10 | Loss: 17905674240.00\n",
      "Epoch  20 | Loss: 1307512.62\n",
      "Epoch  30 | Loss: 984320.38\n",
      "Epoch  40 | Loss: 234282.08\n",
      "Epoch  50 | Loss: 15113.77\n",
      "Epoch  60 | Loss: 13531.70\n",
      "Epoch  70 | Loss: 6246.34\n",
      "Epoch  80 | Loss: 5370.67\n",
      "Epoch  90 | Loss: 3669.27\n",
      "Epoch 100 | Loss: 3074.87\n",
      "Epoch 110 | Loss: 198610.30\n",
      "Epoch 120 | Loss: 4275910.00\n",
      "Epoch 130 | Loss: 28751.39\n",
      "Epoch 140 | Loss: 18302.13\n",
      "Epoch 150 | Loss: 38596.46\n",
      "Epoch 160 | Loss: 23993.80\n",
      "Epoch 170 | Loss: 7438.18\n",
      "Epoch 180 | Loss: 533.50\n",
      "Epoch 190 | Loss: 864.96\n"
     ]
    }
   ],
   "source": [
    "four_layer_model = GCN(3)\n",
    "train(four_layer_model, final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 52133.25\n",
      "Epoch  10 | Loss: 193060.41\n",
      "Epoch  20 | Loss: 351514.22\n",
      "Epoch  30 | Loss: 245.31\n",
      "Epoch  40 | Loss: 151.35\n",
      "Epoch  50 | Loss: 105.38\n",
      "Epoch  60 | Loss: 74.18\n",
      "Epoch  70 | Loss: 66.23\n",
      "Epoch  80 | Loss: 62.79\n",
      "Epoch  90 | Loss: 60.84\n",
      "Epoch 100 | Loss: 59.33\n",
      "Epoch 110 | Loss: 57.90\n",
      "Epoch 120 | Loss: 56.43\n",
      "Epoch 130 | Loss: 54.85\n",
      "Epoch 140 | Loss: 53.11\n",
      "Epoch 150 | Loss: 51.18\n",
      "Epoch 160 | Loss: 49.04\n",
      "Epoch 170 | Loss: 46.06\n",
      "Epoch 180 | Loss: 42.44\n",
      "Epoch 190 | Loss: 38.50\n"
     ]
    }
   ],
   "source": [
    "five_layer_model = GCN(4)\n",
    "train(five_layer_model, final_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1b Adding Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGCN(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers):\n",
    "        super(AttentionGCN, self).__init__()\n",
    "        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # added attention\n",
    "        self.attention = Linear(2 * 768, 1)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.gcn = GCNConv(4, 768)\n",
    "        self.hidden_layers = ModuleList([GCNConv(768, 768) for _ in range(self.num_hidden_layers)])\n",
    "        self.out = Linear(768, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.gcn(x, edge_index).relu()\n",
    "\n",
    "        # Self-attention mechanism\n",
    "        attention_weights = torch.cat((h, h), dim=1)\n",
    "        attention_weights = torch.tanh(self.attention(attention_weights))\n",
    "        attention_weights = torch.nn.functional.softmax(attention_weights, dim=1)\n",
    "        h = h * attention_weights\n",
    "\n",
    "        for _, l in enumerate(self.hidden_layers):\n",
    "            h = l(h, edge_index).relu()\n",
    "        z = self.out(h)\n",
    "        return h, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 28520.75\n",
      "Epoch  10 | Loss: 2151835.00\n",
      "Epoch  20 | Loss: 1873.23\n",
      "Epoch  30 | Loss: 644.09\n",
      "Epoch  40 | Loss: 129.38\n",
      "Epoch  50 | Loss: 68.72\n",
      "Epoch  60 | Loss: 67.01\n",
      "Epoch  70 | Loss: 63.55\n",
      "Epoch  80 | Loss: 60.17\n",
      "Epoch  90 | Loss: 56.18\n",
      "Epoch 100 | Loss: 49.72\n",
      "Epoch 110 | Loss: 47.32\n",
      "Epoch 120 | Loss: 40.66\n",
      "Epoch 130 | Loss: 36.06\n",
      "Epoch 140 | Loss: 30.99\n",
      "Epoch 150 | Loss: 29.23\n",
      "Epoch 160 | Loss: 27.52\n",
      "Epoch 170 | Loss: 27.16\n",
      "Epoch 180 | Loss: 25.84\n",
      "Epoch 190 | Loss: 24.93\n"
     ]
    }
   ],
   "source": [
    "attention_model = AttentionGCN(4)\n",
    "train(attention_model, final_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using other xbar graphs for testing generalization\n",
    "\n",
    "It seems like all the other xbar data was the same?\n",
    "So I am just going to test on the test set to see how it generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.4887, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using attention model because it has the best\n",
    "\n",
    "def test(model, data):\n",
    "    return model(data.x, data.edge_index)\n",
    "\n",
    "mse = torch.nn.MSELoss()\n",
    "mse(attention_model(final_test.x, final_test.edge_index)[1].reshape(-1), final_test.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalized pretty well considering the MSE loss is almost the same for train and test.  I think I could of trained the model a little more as the loss was still decreasing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
